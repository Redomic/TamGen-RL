{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.execv(sys.executable, [sys.executable] + sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0.dev20250524+cu128\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Preparing PDB: 3ny8\n",
      "3ny8 is downloaded\n",
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', tbmf_wrapper=False, seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation_coord', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=1024, max_sentences=None, required_batch_size_multiple=8, dataset_impl=None, gen_subset='gen_8fln', num_shards=1, shard_id=0, path='checkpoints/crossdocked_model/checkpoint_best.pt', remove_bpe=None, quiet=False, model_overrides='{}', results_path=None, beam=20, nbest=20, max_len_a=0, max_len_b=200, min_len=1, match_source_len=False, no_early_stop=False, unnormalized=False, no_beamable_mm=False, lenpen=1, unkpen=0, replace_unk=None, sacrebleu=False, score_reference=False, prefix_size=0, prefix_string=None, tag_prefix_token=False, no_repeat_ngram_size=0, sampling=False, sampling_topk=-1, sampling_topp=-1.0, temperature=1.0, diverse_beam_groups=-1, diverse_beam_strength=0.5, print_alignment=False, scorer='bleu', codegen_pov_replace_unk=True, dump_hyp=None, momentum=0.99, weight_decay=0.0, force_anneal=None, lr_shrink=0.1, warmup_updates=0, data='TamGen_Demo_Data', source_lang='tg', target_lang='m1', lazy_load=False, raw_text=False, left_pad_source='True', left_pad_target='False', max_source_positions=1024, max_target_positions=1024, upsample_primary=1, coord_mode='raw', use_src_coord=True, use_tgt_coord=False, shuffle_input=False, vae=False, sample_beta=1.0, hint=False, hint_rate=0.5, gen_coord_noise=False, gen_rot=False, gen_vae=True, remove_prefix_fragment=False)\n",
      "| [tg] dictionary: 25 types\n",
      "| [m1] dictionary: 962 types\n",
      "| loading model(s) from checkpoints/crossdocked_model/checkpoint_best.pt\n",
      "checkpoints/crossdocked_model/checkpoint_best.pt\n",
      "gpt_model/checkpoint_best.pt\n",
      "| loaded 5 examples from: TamGen_Demo_Data/gen_3ny8.tg-m1.tg\n",
      "| loaded 5 examples from: TamGen_Demo_Data/gen_3ny8.tg-m1.m1\n",
      "| TamGen_Demo_Data gen_3ny8 tg-m1 5 examples\n",
      "| loaded 5 examples from: TamGen_Demo_Data/gen_3ny8.tg-m1.tg.coord\n",
      "‚öôÔ∏è  Starting closed-loop optimization...\n",
      "\n",
      "üöÄ Iteration 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/workspace/workspace/TamGen/fairseq/sequence_generator.py:456: UserWarning: An output with one or more elements was resized since it had shape [5, 20], which does not match the required output shape [4, 20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  torch.topk(\n",
      "/workspace/workspace/TamGen/fairseq/sequence_generator.py:466: UserWarning: An output with one or more elements was resized since it had shape [5, 20], which does not match the required output shape [4, 20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  torch.gather(\n",
      "/workspace/workspace/TamGen/fairseq/search.py:71: UserWarning: An output with one or more elements was resized since it had shape [5, 40], which does not match the required output shape [4, 40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  torch.topk(\n",
      "/workspace/workspace/TamGen/fairseq/sequence_generator.py:456: UserWarning: An output with one or more elements was resized since it had shape [4, 20], which does not match the required output shape [3, 20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  torch.topk(\n",
      "/workspace/workspace/TamGen/fairseq/sequence_generator.py:466: UserWarning: An output with one or more elements was resized since it had shape [4, 20], which does not match the required output shape [3, 20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  torch.gather(\n",
      "/workspace/workspace/TamGen/fairseq/search.py:71: UserWarning: An output with one or more elements was resized since it had shape [4, 40], which does not match the required output shape [3, 40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  torch.topk(\n",
      "/workspace/workspace/TamGen/fairseq/sequence_generator.py:456: UserWarning: An output with one or more elements was resized since it had shape [3, 20], which does not match the required output shape [2, 20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  torch.topk(\n",
      "/workspace/workspace/TamGen/fairseq/sequence_generator.py:466: UserWarning: An output with one or more elements was resized since it had shape [3, 20], which does not match the required output shape [2, 20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  torch.gather(\n",
      "/workspace/workspace/TamGen/fairseq/search.py:71: UserWarning: An output with one or more elements was resized since it had shape [3, 40], which does not match the required output shape [2, 40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  torch.topk(\n",
      "/workspace/workspace/TamGen/fairseq/sequence_generator.py:456: UserWarning: An output with one or more elements was resized since it had shape [2, 20], which does not match the required output shape [1, 20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  torch.topk(\n",
      "/workspace/workspace/TamGen/fairseq/sequence_generator.py:466: UserWarning: An output with one or more elements was resized since it had shape [2, 20], which does not match the required output shape [1, 20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  torch.gather(\n",
      "/workspace/workspace/TamGen/fairseq/search.py:71: UserWarning: An output with one or more elements was resized since it had shape [2, 40], which does not match the required output shape [1, 40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  torch.topk(\n",
      "  5%|‚ñå         | 1/20 [00:03<01:01,  3.21s/it]/workspace/workspace/TamGen/fairseq/search.py:71: UserWarning: An output with one or more elements was resized since it had shape [1, 40], which does not match the required output shape [5, 40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  torch.topk(\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [00:15<00:21,  1.79s/it]/workspace/workspace/TamGen/fairseq/sequence_generator.py:456: UserWarning: An output with one or more elements was resized since it had shape [3, 20], which does not match the required output shape [1, 20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  torch.topk(\n",
      "/workspace/workspace/TamGen/fairseq/sequence_generator.py:466: UserWarning: An output with one or more elements was resized since it had shape [3, 20], which does not match the required output shape [1, 20]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  torch.gather(\n",
      "/workspace/workspace/TamGen/fairseq/search.py:71: UserWarning: An output with one or more elements was resized since it had shape [3, 40], which does not match the required output shape [1, 40]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:31.)\n",
      "  torch.topk(\n",
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [00:36<00:01,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid SMILES and latents after all seeds: 95\n",
      "Preview of valid SMILES: ['Cc1ccc(OC[C@@H](O)[C@H](C)NC(C)C)c2c1CCC2', 'CC(C)CCC[C@@H](C)[C@H]1CC[C@H]2[C@@H]3CC=C4C[C@@H](O)CC[C@]4(C)[C@H]3CC[C@]12C', 'CCCCCCCC/C=C\\\\CCCCCCCC(=O)OC[C@H](O)CO', 'CCCCCCCC/C=C\\\\CCCCCCCC(=O)O', 'OCCOCCOCCO', 'Cc1ccc(OC[C@@H](O)[C@H](C)NC(C)C)c2c1CCC2', 'CC(C)CCC[C@@H](C)[C@H]1CC[C@H]2[C@@H]3CC=C4C[C@@H](O)CC[C@]4(C)[C@H]3CC[C@]12C', 'CCCCCCCC/C=C\\\\CCCCCCCC(=O)OC[C@H](O)CO', 'CCCCCCCC/C=C\\\\CCCCCCCC(=O)O', 'OCCOCCOCCO']\n",
      "Preview of latent vector shapes: [(256,), (256,), (256,), (256,), (256,), (256,), (256,), (256,), (256,), (256,)]\n",
      "Unique SMILES this iteration: 5 / 95\n",
      "üìä Optimizing latent space...\n",
      "[DEBUG] Model created on device: cpu\n",
      "[DEBUG] z_tensor device: cpu, r_tensor device: cpu\n",
      "[DEBUG] Cleared z_list and r_list after training.\n",
      "Model parameters devices: [device(type='cpu'), device(type='cpu'), device(type='cpu'), device(type='cpu')]\n",
      "Stored tensors devices: []\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m demo\u001b[38;5;241m.\u001b[39mreload_data(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m pdb_id\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# === Run Closed-Loop Optimization ===\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m final_smiles \u001b[38;5;241m=\u001b[39m \u001b[43mdemo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mm_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Number of molecules per iteration\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Number of closed-loop optimization steps\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# Latent space dimensionality (set to your model's config)\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Centroid shift parameter\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# How many top molecules to use for shifting\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_sas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# Reward hyperparameters\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_logp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Number of random seeds (first iteration)\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# === Save or Analyze Results ===\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal set of SMILES (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(final_smiles)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m molecules):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/workspace/TamGen/TamGen_RL.py:149\u001b[0m, in \u001b[0;36mTamGenRL.sample\u001b[0;34m(self, m_sample, num_iter, latent_dim, alpha, top_k, lambda_sas, lambda_logp, lambda_mw, maxseed, use_cuda, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m docking_scores \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(smiles_list)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìä Optimizing latent space...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 149\u001b[0m z_shifted, rewards, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcentroid_shift_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz_vectors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43msmiles_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocking_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshift_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_sas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_sas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_logp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_logp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_mw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoise_sigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   ‚úî Optimization complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# 5. Save Outputs\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/workspace/TamGen/feedback/centroid_optimizer.py:63\u001b[0m, in \u001b[0;36mcentroid_shift_optimize\u001b[0;34m(z_vectors, smiles_list, docking_scores, latent_dim, lambda_sas, lambda_logp, lambda_mw, top_k, shift_alpha, noise_sigma)\u001b[0m\n\u001b[1;32m     61\u001b[0m         stored_devices\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStored tensors devices:\u001b[39m\u001b[38;5;124m\"\u001b[39m, stored_devices)\n\u001b[0;32m---> 63\u001b[0m direction \u001b[38;5;241m=\u001b[39m \u001b[43mreward_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_centroid_shift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m reward_model\u001b[38;5;241m.\u001b[39mz_list\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     66\u001b[0m reward_model\u001b[38;5;241m.\u001b[39mr_list\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m/workspace/workspace/TamGen/feedback/reward_model.py:71\u001b[0m, in \u001b[0;36mLatentRewardModel.get_centroid_shift\u001b[0;34m(self, top_k)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_centroid_shift\u001b[39m(\u001b[38;5;28mself\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m---> 71\u001b[0m     top_z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_k_z\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     72\u001b[0m     all_z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_list)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(top_z, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(all_z, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/workspace/workspace/TamGen/feedback/reward_model.py:66\u001b[0m, in \u001b[0;36mLatentRewardModel.top_k_z\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtop_k_z\u001b[39m(\u001b[38;5;28mself\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m---> 66\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mz_list\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     67\u001b[0m     top_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(preds)[\u001b[38;5;241m-\u001b[39mk:]\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_list[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m top_indices]\n",
      "File \u001b[0;32m~/miniconda3/envs/TamGen/lib/python3.9/site-packages/numpy/_core/shape_base.py:444\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    442\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [asanyarray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    446\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "from TamGen_RL import TamGenRL\n",
    "from utils import prepare_pdb_data, prepare_pdb_data_center, filter_generated_cmpd\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "# === Setup TamGenRL ===\n",
    "pdb_id = \"3ny8\"\n",
    "print(f\"üìÑ Preparing PDB: {pdb_id}\")\n",
    "prepare_pdb_data(pdb_id)\n",
    "\n",
    "demo = TamGenRL(\n",
    "    data=\"TamGen_Demo_Data\",\n",
    "    ckpt=\"checkpoints/crossdocked_model/checkpoint_best.pt\",\n",
    "    use_conditional=True\n",
    ")\n",
    "demo.reload_data(subset=\"gen_\" + pdb_id.lower())\n",
    "\n",
    "# === Run Closed-Loop Optimization ===\n",
    "final_smiles = demo.sample(\n",
    "    m_sample=50000,         # Number of molecules per iteration\n",
    "    num_iter=5,           # Number of closed-loop optimization steps\n",
    "    latent_dim=256,       # Latent space dimensionality (set to your model's config)\n",
    "    alpha=0.5,            # Centroid shift parameter\n",
    "    top_k=50,             # How many top molecules to use for shifting\n",
    "    lambda_sas=0.3,       # Reward hyperparameters\n",
    "    lambda_logp=0.1,\n",
    "    lambda_mw=0.1,\n",
    "    maxseed=20,           # Number of random seeds (first iteration)\n",
    "    use_cuda=True\n",
    ")\n",
    "\n",
    "# === Save or Analyze Results ===\n",
    "print(f\"\\nFinal set of SMILES ({len(final_smiles)} molecules):\")\n",
    "for smi in final_smiles:\n",
    "    print(smi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reward_files = sorted(glob.glob('latent_logs/rewards_iter_*.tsv'))\n",
    "means, maxs, medians = [], [], []\n",
    "\n",
    "for f in reward_files:\n",
    "    rewards = []\n",
    "    with open(f) as fin:\n",
    "        for line in fin:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                rewards.append(float(parts[1]))\n",
    "    if rewards:\n",
    "        rewards = np.array(rewards)\n",
    "        means.append(rewards.mean())\n",
    "        maxs.append(rewards.max())\n",
    "        medians.append(np.median(rewards))\n",
    "\n",
    "plt.plot(means, label='Mean')\n",
    "plt.plot(maxs, label='Max')\n",
    "plt.plot(medians, label='Median')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward Statistics Across Iterations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TamGen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
